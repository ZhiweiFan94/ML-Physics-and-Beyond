{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "Competition Description\n",
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster. Take this example:\n",
    "\n",
    "\n",
    "The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n",
    "\n",
    "In this competition, you’re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one’s aren’t. You’ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n",
    "\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n",
    "\n",
    "Submissions are evaluated using F1 between the predicted and expected answers.\n",
    "\n",
    "F1 is calculated as follows:\n",
    "\n",
    "𝐹1=2∗𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑟𝑒𝑐𝑎𝑙𝑙 / (𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+𝑟𝑒𝑐𝑎𝑙𝑙)\n",
    "\n",
    "where:\n",
    "\n",
    "𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=𝑇𝑃/(𝑇𝑃+𝐹𝑃)\n",
    "\n",
    "𝑟𝑒𝑐𝑎𝑙𝑙=𝑇𝑃/(𝑇𝑃+𝐹𝑁)\n",
    "\n",
    "and:\n",
    "\n",
    "True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n",
    "\n",
    "False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n",
    "\n",
    "False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means we need to deal with text and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of models\n",
    "\n",
    "We are taking a model for a binary classification task, here are some investigation regarding this work:\n",
    "\n",
    "BERT and its Variants: BERT is particularly good at understanding the context of a word in a sentence, which can be useful for tasks like sentiment analysis where the meaning of words can be highly context-dependent. Its variants like RoBERTa or ALBERT might offer improved performance or efficiency. **Seems BERT is a good start to try**\n",
    "\n",
    "DistilBERT: If computational resources or inference time are a concern, DistilBERT offers a good balance between performance and efficiency, as it is a distilled version of BERT that retains most of its capabilities. **Skip this for the time being, let us consider pure version of BERT first**\n",
    "\n",
    "GPT-3 or GPT-4: If your task can benefit from a large-scale pre-trained model and you have access to it, GPT-3 or GPT-4 can be fine-tuned for binary classification tasks. These models can be particularly powerful if your task involves creative language or requires a deep understanding of nuanced text. **Not very relevant to this task**\n",
    "\n",
    "Fine-tuning vs. Feature-based Approach: With models like BERT, you can either fine-tune the entire model on your task or use it as a feature extractor where only a simple classifier is trained on top of the BERT features. Fine-tuning generally offers better performance but requires more computational resources.\n",
    "\n",
    "Data Efficiency: If you have a small dataset, you might want to consider models that are known for being data-efficient. Techniques like few-shot learning with GPT-3 or GPT-4, or using a model pre-trained in a similar domain to your task, can be beneficial.\n",
    "\n",
    "Transfer Learning and Domain-Specific Models: If there's a pre-trained model that's been trained on a corpus relevant to your task (like legal documents, medical reports, etc.), using that model could lead to better performance as it's already familiar with the kind of language used in your domain.\n",
    "\n",
    "Here, we got relatively small size of training set, therefore BERT maybe the first one we want to try.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clean for BERT model\n",
    "When fine-tuning a BERT model, it's generally a good idea to clean and preprocess your text data to ensure that it is in a format that is compatible with the model and conducive to effective learning. Here are some common preprocessing steps for training a BERT model.\n",
    "\n",
    "Text Cleaning: Depending on your dataset, you might want to remove unnecessary characters, such as HTML tags, special characters, or extra whitespace. You should also consider whether to convert the text to lowercase, as BERT models are available in both cased and uncased versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  our deeds are the reason of this earthquake ma...  \n",
      "1              forest fire near la ronge sask canada  \n",
      "2  all residents asked to shelter in place are be...  \n",
      "3  people receive wildfires evacuation orders in ...  \n",
      "4  just got sent this photo from ruby alaska as s...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove usernames (mentions)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (just the symbol, not the text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove non-alphabetic characters and keep only the words\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Assuming the dataset is in a CSV file and the text is in a column named 'text'\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Clean the text column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Show the first few rows of the cleaned text\n",
    "print(df[['text', 'cleaned_text']].head())\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "df.to_csv('cleaned_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding of BERT and its controlling parameters\n",
    "\n",
    "### BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a model that uses the Transformer architecture. It was introduced by researchers at Google in 2018 and has since become one of the most popular and influential models in natural language processing (NLP).\n",
    "\n",
    "Here's a high-level overview of how the BERT model works:\n",
    "\n",
    "Input Representation: BERT takes as input a sequence of tokens (words or subwords) and converts them into vectors using an embedding layer. It also adds special tokens like [CLS] and [SEP] to the sequence and uses positional embeddings to capture the order of the tokens.\n",
    "\n",
    "Transformer Encoder: The core of the BERT model is a stack of Transformer encoder layers. Each layer consists of two main components: a multi-head self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different tokens in the sequence relative to each other, and the feed-forward network processes the output of the attention mechanism.\n",
    "\n",
    "Self-Attention: In the self-attention mechanism, each token in the input sequence is transformed into a query, key, and value vector. The model then computes attention scores by taking the dot product of the query vector of each token with the key vectors of all other tokens. These scores determine how much attention each token should pay to every other token in the sequence. The attention scores are then used to compute a weighted sum of the value vectors, which becomes the output of the attention mechanism.\n",
    "\n",
    "Feed-Forward Network: The output of the self-attention mechanism is then passed through a feed-forward neural network, which applies additional transformations to the data.\n",
    "\n",
    "Output: The output of the final Transformer encoder layer can be used for various NLP tasks. For example, in classification tasks, the output corresponding to the [CLS] token is often used as the representation of the entire sequence and passed through additional layers to produce the final class predictions.\n",
    "\n",
    "BERT's ability to capture the context of each token in a sequence bidirectionally (i.e., considering both the preceding and following tokens) is one of its key strengths. This allows it to understand the meaning of words in context more effectively than previous models that processed text in a unidirectional manner.\n",
    "\n",
    "### Training process\n",
    "Tokenization: Use the BERT tokenizer to tokenize your text. This involves splitting the text into words or subwords (tokens) and converting each token into its corresponding ID in the BERT vocabulary. The tokenizer will also add special tokens like [CLS] and [SEP] as needed.\n",
    "\n",
    "Padding and Truncation: Since BERT models require fixed-length input sequences, you'll need to pad shorter sequences and truncate longer ones to a specified maximum length. The BERT tokenizer can handle this for you.\n",
    "\n",
    "Attention Masks: Generate attention masks to tell the model which tokens are actual words and which are padding tokens. This is important for the model to correctly interpret the input sequences.\n",
    "\n",
    "Label Encoding: If you're working on a classification task, you'll need to encode your labels into a format that the model can understand (e.g., converting categorical labels into numerical IDs).\n",
    "\n",
    "### Controlling parameters (which used in the coding below)\n",
    "- add_special_tokens=True:\n",
    "\n",
    "This parameter tells the tokenizer to add special tokens to the beginning and end of each sequence. For BERT, these are typically [CLS] at the beginning and [SEP] at the end. The [CLS] token is used for classification tasks, and the [SEP] token is used to separate different sentences or segments within a single sequence.\n",
    "\n",
    "- return_attention_mask=True:\n",
    "\n",
    "This parameter tells the tokenizer to return the attention mask, which we discussed earlier. This is necessary for the BERT model to distinguish between real tokens and padding tokens.\n",
    "\n",
    "- pad_to_max_length=True:\n",
    "\n",
    "This parameter tells the tokenizer to pad all sequences to a specified maximum length (max_length). This is important because BERT models require all input sequences to be of the same length.\n",
    "\n",
    "- input_ids = inputs['input_ids']:\n",
    "\n",
    "input_ids are the tokenized representations of your input text. The BERT tokenizer converts each token (word or subword) in your text into a unique integer ID. These IDs are used by the BERT model to look up the corresponding embeddings (vector representations) of each token. The inputs['input_ids'] is a dictionary key that retrieves these tokenized input IDs from the tokenizer output.\n",
    "\n",
    "- attention_masks = inputs['attention_mask']:\n",
    "\n",
    "attention_masks are used to tell the model which tokens in the input are actual words and which ones are padding tokens. This is important because BERT models are trained on fixed-length sequences, and not all input sequences are the same length. Padding tokens (usually represented by the ID 0) are added to the end of shorter sequences to make them all the same length. The attention mask is a binary mask that indicates which tokens are padding (0) and which are real words (1).\n",
    "\n",
    "- output_attentions=False:\n",
    "\n",
    "This parameter is used when loading the BERT model. Setting it to False means that the model will not return the attention weights. The attention weights are used to understand how the model is focusing on different parts of the input sequence when making predictions. If you don't need this information, you can set this parameter to False to save memory and computation.\n",
    "\n",
    "- output_hidden_states=False:\n",
    "\n",
    "Similar to output_attentions, this parameter is used when loading the BERT model. Setting it to False means that the model will not return the hidden states of each layer. The hidden states can be used for more advanced analysis or for creating more complex models, but if you're just doing simple classification, you can set this to False to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_train.csv')\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sequences in the dataset\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    df['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,  # Choose a max_length that suits your dataset\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "# Get the input IDs and attention masks from the tokenizer output\n",
    "input_ids = inputs['input_ids']\n",
    "attention_masks = inputs['attention_mask']\n",
    "\n",
    "# Get the labels from the dataframe\n",
    "labels = torch.tensor(df['target'].values)\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,  # Number of output labels--2 for binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Up Data Loaders\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "AdamW is a popular optimizer used in training deep learning models, especially in the context of fine-tuning models like BERT. It is a variant of the Adam optimizer that incorporates weight decay, a technique used to regularize and prevent overfitting in the model.\n",
    "\n",
    "Here's why AdamW is commonly used for fine-tuning BERT:\n",
    "\n",
    "Adaptive Learning Rates: AdamW, like Adam, computes adaptive learning rates for each parameter. This means that it adjusts the learning rate based on the history of gradients for each parameter, which can lead to more effective and faster training compared to optimizers with a fixed learning rate.\n",
    "\n",
    "Weight Decay: AdamW incorporates weight decay in a way that is more compatible with adaptive learning rate algorithms. Weight decay is a form of regularization that helps prevent the weights from growing too large, which can reduce overfitting. In traditional optimizers like SGD with momentum, weight decay is applied directly to the weights, but in AdamW, it is decoupled from the gradient updates, which can lead to better performance in practice.\n",
    "\n",
    "Stability: AdamW includes a term called epsilon (eps) that helps improve numerical stability during optimization. This can be particularly important when working with deep neural networks, where small numerical errors can accumulate and lead to instability.\n",
    "\n",
    "While AdamW is a popular choice for fine-tuning BERT and other transformer-based models, there are other optimizers you could consider:\n",
    "\n",
    "SGD (Stochastic Gradient Descent): A simple and classic optimizer that can work well with a properly tuned learning rate and momentum. It may require more epochs to converge compared to AdamW.\n",
    "\n",
    "RMSprop: Similar to Adam in that it maintains a moving average of squared gradients to adapt the learning rate for each parameter, but does not have a bias correction term.\n",
    "\n",
    "Adam: The original version of AdamW, which also adapts learning rates based on the history of gradients but handles weight decay differently.\n",
    "\n",
    "Each optimizer has its strengths and weaknesses, and the best choice can depend on the specific task and dataset. However, AdamW is often recommended for fine-tuning BERT due to its balance of efficiency, stability, and effectiveness in handling sparse gradients and adaptive learning rates.\n",
    "\n",
    "\n",
    "### Discussion of optimizers\n",
    "\n",
    "Regarding whether AdamW is better than other optimizers, it's not always the case that AdamW is universally superior. Its effectiveness can depend on the specific task, model architecture, and dataset. However, AdamW is often preferred for training deep learning models like BERT and CNNs because it combines the benefits of adaptive learning rates (from Adam) with a more effective handling of weight decay. This can lead to better generalization and faster convergence in many cases.\n",
    "\n",
    "For CNNs, AdamW can also be a good choice, especially in scenarios where you're dealing with sparse gradients or require adaptive learning rates. However, traditional optimizers like SGD with momentum are also commonly used for training CNNs and can perform very well, particularly with a well-tuned learning rate schedule.\n",
    "\n",
    "In summary, while AdamW is a powerful optimizer that is well-suited for many deep learning tasks, the best optimizer for a given problem depends on the specific characteristics of the task and the model. It's often a good idea to experiment with different optimizers and learning rate schedules to find the best combination for your particular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer & Learning Rate Scheduler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Set up the optimizer.\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,    # This is the learning rate recommended by the BERT authors\n",
    "                  eps = 1e-8    # This is the default epsilon value in AdamW\n",
    "                )\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does 'Scheduler' do?\n",
    "The learning rate scheduler is used to adjust the learning rate during training, typically reducing it over time. This can help improve the performance of the model and ensure that it converges to a good solution. The scheduler you're using, get_linear_schedule_with_warmup, is a common choice for fine-tuning BERT and other transformer models. It starts with a \"warm-up\" phase, where the learning rate increases linearly from 0 to the initial learning rate (set when configuring the optimizer). After the warm-up phase, the learning rate decreases linearly from the initial learning rate to 0 over the remaining training steps. This approach helps stabilize the training process in the early stages and leads to better final performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions\n",
    "**flat_accuracy function**:\n",
    "The flat_accuracy function is used to calculate the accuracy of the model's predictions. Here's how it works:\n",
    "\n",
    "- preds is a 2D array where each row represents a prediction made by the model for a single input, and each column represents the probability of each class. The shape of preds is (number of examples, number of classes).\n",
    "\n",
    "- np.argmax(preds, axis=1) finds the index of the maximum value in each row, which corresponds to the predicted class. This converts the probabilities into class predictions.\n",
    "- pred_flat is a 1D array containing the predicted classes for all input examples.\n",
    "- labels is a 2D array containing the true labels, which are then flattened into a 1D array labels_flat.\n",
    "- The function then compares pred_flat and labels_flat element-wise to determine how many predictions match the true labels. The sum of matches is divided by the total number of examples to calculate the accuracy.\n",
    "\n",
    "**Explanation of model.cuda()**:\n",
    "\n",
    "model.cuda() is a method that moves the model's parameters and buffers to the GPU. This allows the model to take advantage of the parallel processing power of the GPU, which can significantly speed up training and inference.\n",
    "If you've only worked with CPU computation before, using .cuda() is a way to switch to GPU computation. Note that you'll need a CUDA-capable GPU and the appropriate CUDA software installed for this to work. If you don't have a GPU, you can remove this line, and the model will run on the CPU by default. **You will be introduced how to setup CUDA environment later**\n",
    "\n",
    "**model.train() and training loop**\n",
    "\n",
    "model.train() is a method that sets the model to training mode. This is important because some layers, like dropout and batch normalization, behave differently during training than during evaluation. By calling model.train(), you're telling the model that it should prepare for training.The for loop after model.train() iterates over the training data in batches. Here's what happens inside the loop:\n",
    "\n",
    "- Data Loading: Each batch of data is unpacked into input IDs (b_input_ids), attention masks (b_input_mask), and labels (b_labels). These are then moved to the GPU if available.\n",
    "- Zero Gradient: The gradients are zeroed out using model.zero_grad() to ensure that they don't accumulate across batches.\n",
    "- Forward Pass: The model performs a forward pass on the input data (b_input_ids and b_input_mask) and computes the loss based on the predictions and true labels (b_labels).\n",
    "- Backward Pass: The loss.backward() call computes the gradients of the loss with respect to the model parameters.\n",
    "- Gradient Clipping: torch.nn.utils.clip_grad_norm_() is used to prevent the gradients from becoming too large, which can cause numerical instability. The norm is a measure of the vector's magnitude, typically calculated using the L2 norm (Euclidean norm), which is the square root of the sum of the squares of all elements in the vector. If the norm of the gradient vector exceeds the specified threshold (in this case, 1), we scale down the entire vector so that its norm becomes equal to the threshold. This is done by dividing each element of the gradient vector by the ratio of the norm to the threshold. So, if the norm of the gradient vector is 1.9 and the threshold is 1, each element of the gradient vector is scaled down by a factor of 1/1.9. This ensures that the scaled gradient vector has a norm of 1 while preserving the direction of the original gradient vector. The elements of the gradient vector are scaled down proportionally, not just the highest value.\n",
    "- Parameter Update: The optimizer (optimizer.step()) updates the model parameters based on the computed gradients.\n",
    "- Learning Rate Update: The learning rate scheduler (scheduler.step()) adjusts the learning rate according to the schedule.\n",
    "\n",
    "This process is repeated for each batch in the dataset, and for each epoch (full pass over the dataset), allowing the model to learn from the data and adjust its parameters to minimize the loss.\n",
    "\n",
    "\n",
    "### Validation Phase \n",
    "- Add batch to GPU:\n",
    "\n",
    "The line batch = tuple(t.cuda() for t in batch) moves each tensor in the batch (input IDs, attention masks, and labels) to the GPU. This is done to accelerate computation since GPUs are much faster than CPUs for the matrix operations involved in deep learning.\n",
    "\n",
    "The batch is in tuple format because the data loader typically returns batches as tuples or lists of tensors. The tuple format is used here to ensure that the structure of the batch remains unchanged when moving it to the GPU.\n",
    "\n",
    "- Logits:\n",
    "\n",
    "In the context of neural networks, logits are the raw, unnormalized scores output by the last layer of the model before any activation function like softmax is applied. These scores are then typically passed through a softmax function to convert them into probabilities for each class.\n",
    "outputs[0] refers to the logits because, in the PyTorch implementation of BERT, the model returns a tuple where the first element (outputs[0]) is the logits for each input sequence. \n",
    "\n",
    "- Move logits and labels to CPU:\n",
    "\n",
    "Logits and labels are moved to the CPU (logits.detach().cpu().numpy() and b_labels.to('cpu').numpy()) for further processing (like calculating the F1 score) that doesn't require the computational power of the GPU. This is done because some operations, like converting tensors to NumPy arrays, can only be done on CPU tensors. Additionally, moving data off the GPU can help free up GPU memory for other computations.\n",
    "\n",
    "- Weighted average for F1 score:\n",
    "\n",
    "The average='weighted' parameter in the f1_score function specifies that the F1 scores for each class should be weighted by the number of true instances for each class. This means that the F1 score for each class is multiplied by the number of true instances in that class, and then the average is taken. This approach is useful in imbalanced datasets, where some classes might have significantly more instances than others. It ensures that the F1 score takes into account the class imbalance by giving more weight to the classes with more instances.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  215.    Elapsed: 0:00:33.\n",
      "  Batch 80  of  215.    Elapsed: 0:01:06.\n",
      "  Batch 120  of  215.    Elapsed: 0:01:40.\n",
      "  Batch 160  of  215.    Elapsed: 0:02:13.\n",
      "  Batch 200  of  215.    Elapsed: 0:02:46.\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:02:58\n",
      "\n",
      "Running Validation with F1 score...\n",
      "F1 Score: 0.83\n",
      "Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  215.    Elapsed: 0:00:33.\n",
      "  Batch 80  of  215.    Elapsed: 0:01:07.\n",
      "  Batch 120  of  215.    Elapsed: 0:01:40.\n",
      "  Batch 160  of  215.    Elapsed: 0:02:14.\n",
      "  Batch 200  of  215.    Elapsed: 0:02:47.\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "Running Validation with F1 score...\n",
      "F1 Score: 0.83\n",
      "Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  215.    Elapsed: 0:00:33.\n",
      "  Batch 80  of  215.    Elapsed: 0:01:07.\n",
      "  Batch 120  of  215.    Elapsed: 0:01:40.\n",
      "  Batch 160  of  215.    Elapsed: 0:02:14.\n",
      "  Batch 200  of  215.    Elapsed: 0:02:47.\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "Running Validation with F1 score...\n",
      "F1 Score: 0.83\n",
      "Validation took: 0:00:07\n",
      "\n",
      "Validation complete!\n"
     ]
    }
   ],
   "source": [
    "#Training loops\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function for formatting the elapsed time.\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "# Move the model to the GPU.\n",
    "model.cuda()\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}.    Elapsed: {elapsed}.')\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_input_mask = batch[1].cuda()\n",
    "        b_labels = batch[2].cuda()\n",
    "        \n",
    "        # Clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()        \n",
    "        \n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Accumulate the training loss over all of the batches \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {format_time(time.time() - t0)}\")    \n",
    "    print(\"\\nRunning Validation with F1 score...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    # Flatten the predictions and true labels\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Convert logits to predicted class (0 or 1) using argmax\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(flat_true_labels, flat_predictions, average='weighted') # 'weighted' accounts for label imbalance\n",
    "\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Validation took: {format_time(time.time() - t0)}\\n\")\n",
    "\n",
    "print(\"Validation complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_bert_model3\\\\tokenizer_config.json',\n",
       " './my_bert_model3\\\\special_tokens_map.json',\n",
       " './my_bert_model3\\\\vocab.txt',\n",
       " './my_bert_model3\\\\added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./my_bert_model3')\n",
    "tokenizer.save_pretrained('./my_bert_model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('my_bert_model3')\n",
    "model = BertForSequenceClassification.from_pretrained('my_bert_model3')\n",
    "\n",
    "input_text = df_test['text'].tolist()\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "predictions_np = predictions.numpy()\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_np, columns=['target'])\n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "df_output['id'] = df_test['id']\n",
    "df_output['target'] = predictions_df['target']\n",
    "df_output.to_csv('submit3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal notes： 关于BERT模型其他类型的fine-tune\n",
    "BERT模型可以用于多种自然语言处理任务，包括但不限于二分类问题。对于不同的任务，模型的微调和输出层可能会有所不同：\n",
    "\n",
    "多分类问题：如果任务是多分类而不是二分类，你可以简单地修改模型的输出层，使其具有与类别数量相对应的输出神经元。例如，如果有10个类别，那么输出层应该有10个神经元，并且使用softmax激活函数来将输出转换为概率分布。\n",
    "\n",
    "上下文预测：如果任务是根据上下文预测单词（例如，填空任务），你可以使用BERT的Masked Language Model (MLM) 功能。在这种情况下，你会在输入文本中随机遮盖一些单词，然后训练模型来预测这些遮盖的单词。输出层需要调整为词汇表大小的输出，以预测每个遮盖单词的概率分布。\n",
    "\n",
    "问答任务：对于问答任务，你可以使用BERT来同时预测答案在文本中的开始和结束位置。这通常涉及到在BERT的基础上添加两个输出层，一个用于预测答案开始的位置，另一个用于预测答案结束的位置。每个输出层都会输出一个概率分布，表示答案开始或结束的每个位置的概率。\n",
    "\n",
    "对于以上任何任务，模型的其余部分（即BERT的编码器层）通常保持不变，只需要根据具体任务调整输出层和损失函数。此外，你可能还需要根据任务的具体需求调整输入数据的格式和预处理步骤。\n",
    "\n",
    "对于问答（QA）任务，模型通常被训练来预测答案在文本中的开始和结束位置。这里以BERT为例来详细说明训练过程：\n",
    "\n",
    "数据准备：\n",
    "\n",
    "首先，你需要准备一个问答数据集，其中每个样本包含一个问题、一个包含答案的上下文文本（例如，一个段落），以及答案在上下文中的开始和结束位置。\n",
    "对于每个样本，你将问题和上下文文本拼接在一起，中间用一个特殊分隔符（如[SEP]）分隔，同时在序列的开始添加一个特殊的分类标记（如[CLS]）。\n",
    "模型架构：\n",
    "\n",
    "使用BERT模型对拼接后的问题和上下文文本进行编码，得到序列的表示。\n",
    "在BERT的输出层之上，添加两个线性层，一个用于预测答案的开始位置，另一个用于预测答案的结束位置。这两个线性层的输出维度等于序列长度，每个位置的输出值表示该位置是答案开始或结束的概率。\n",
    "训练过程：\n",
    "\n",
    "在训练过程中，模型的目标是最小化预测的开始和结束位置与真实开始和结束位置之间的差距。这通常通过交叉熵损失函数来实现。\n",
    "对于每个样本，计算开始位置和结束位置的损失，然后将这两个损失相加得到总损失。使用梯度下降算法（如Adam）来更新模型参数，以最小化总损失。\n",
    "预测和评估：\n",
    "\n",
    "在预测阶段，模型接收一个问题和一个上下文文本，输出答案的开始和结束位置的概率分布。\n",
    "选择概率最高的位置作为答案的开始和结束位置。然后从上下文文本中提取出这个范围内的文本作为预测答案。\n",
    "模型的性能通常通过评估指标，如精确率（Precision）、召回率（Recall）和F1分数来衡量，这些指标考虑了预测答案与真实答案的重叠程度。\n",
    "这就是使用BERT进行问答任务的基本训练过程。需要注意的是，实际应用中可能需要对这个过程进行调整，以适应特定的数据集和任务要求。\n",
    "\n",
    "\n",
    "在实际应用中，确实存在这样的问答任务，其中只提供问题，而没有给出包含答案的上下文。这种任务通常被称为开放域问答（Open-domain Question Answering），它比传统的基于上下文的问答（Context-based Question Answering）更具挑战性。开放域问答通常涉及以下几个步骤：\n",
    "\n",
    "文档检索（Document Retrieval）：给定一个问题，系统首先需要从一个大型的文档集合（如维基百科）中检索出相关的文档或段落。这个步骤通常使用信息检索（Information Retrieval）技术来实现，如倒排索引（Inverted Index）、TF-IDF、BM25等。\n",
    "\n",
    "候选生成（Candidate Generation）：从检索到的文档中提取出可能包含答案的文本片段作为答案候选。这可以通过简单的启发式方法来实现，例如选择包含问题中关键词的句子或段落。\n",
    "\n",
    "答案抽取（Answer Extraction）：对于每个候选文本，使用类似于上述基于上下文的问答模型（如基于BERT的模型）来预测答案的开始和结束位置。这个步骤通常需要将问题和候选文本拼接起来，作为模型的输入。\n",
    "\n",
    "答案排序（Answer Ranking）：对所有候选答案进行排序，选择最可能的答案作为最终输出。排序可以基于模型预测的概率，也可以结合其他特征，如候选文本的相关性得分、答案的置信度等。\n",
    "\n",
    "开放域问答需要综合运用信息检索、自然语言处理和机器学习的技术。随着预训练语言模型和检索技术的发展，开放域问答系统的性能正在逐渐提高，但仍然是自然语言处理领域的一个活跃研究方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
