{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Disaster Tweets\n",
    "\n",
    "Competition Description\n",
    "\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "\n",
    "The ubiquitousness of smartphones enables people to announce an emergency theyâ€™re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "\n",
    "But, itâ€™s not always clear whether a personâ€™s words are actually announcing a disaster. Take this example:\n",
    "\n",
    "\n",
    "The author explicitly uses the word â€œABLAZEâ€ but means it metaphorically. This is clear to a human right away, especially with the visual aid. But itâ€™s less clear to a machine.\n",
    "\n",
    "In this competition, youâ€™re challenged to build a machine learning model that predicts which Tweets are about real disasters and which oneâ€™s arenâ€™t. Youâ€™ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n",
    "\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n",
    "\n",
    "Submissions are evaluated using F1 between the predicted and expected answers.\n",
    "\n",
    "F1 is calculated as follows:\n",
    "\n",
    "ğ¹1=2âˆ—ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›âˆ—ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™ / (ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›+ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™)\n",
    "\n",
    "where:\n",
    "\n",
    "ğ‘ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ)\n",
    "\n",
    "ğ‘Ÿğ‘’ğ‘ğ‘ğ‘™ğ‘™=ğ‘‡ğ‘ƒ/(ğ‘‡ğ‘ƒ+ğ¹ğ‘)\n",
    "\n",
    "and:\n",
    "\n",
    "True Positive [TP] = your prediction is 1, and the ground truth is also 1 - you predicted a positive and that's true!\n",
    "\n",
    "False Positive [FP] = your prediction is 1, and the ground truth is 0 - you predicted a positive, and that's false.\n",
    "\n",
    "False Negative [FN] = your prediction is 0, and the ground truth is 1 - you predicted a negative, and that's false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword location                                               text  \\\n",
       "0    1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1    4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2    5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3    6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4    7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "5    8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...   \n",
       "6   10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...   \n",
       "7   13     NaN      NaN  I'm on top of the hill and I can see a fire in...   \n",
       "8   14     NaN      NaN  There's an emergency evacuation happening now ...   \n",
       "9   15     NaN      NaN  I'm afraid that the tornado is coming to our a...   \n",
       "10  16     NaN      NaN        Three people died from the heat wave so far   \n",
       "11  17     NaN      NaN  Haha South Tampa is getting flooded hah- WAIT ...   \n",
       "12  18     NaN      NaN  #raining #flooding #Florida #TampaBay #Tampa 1...   \n",
       "13  19     NaN      NaN            #Flood in Bago Myanmar #We arrived Bago   \n",
       "14  20     NaN      NaN  Damage to school bus on 80 in multi car crash ...   \n",
       "15  23     NaN      NaN                                     What's up man?   \n",
       "16  24     NaN      NaN                                      I love fruits   \n",
       "17  25     NaN      NaN                                   Summer is lovely   \n",
       "18  26     NaN      NaN                                  My car is so fast   \n",
       "19  28     NaN      NaN                       What a goooooooaaaaaal!!!!!!   \n",
       "\n",
       "    target  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "5        1  \n",
       "6        1  \n",
       "7        1  \n",
       "8        1  \n",
       "9        1  \n",
       "10       1  \n",
       "11       1  \n",
       "12       1  \n",
       "13       1  \n",
       "14       1  \n",
       "15       0  \n",
       "16       0  \n",
       "17       0  \n",
       "18       0  \n",
       "19       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('train.csv')\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It means we need to deal with text and target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The choice of models\n",
    "\n",
    "We are taking a model for a binary classification task, here are some investigation regarding this work:\n",
    "\n",
    "BERT and its Variants: BERT is particularly good at understanding the context of a word in a sentence, which can be useful for tasks like sentiment analysis where the meaning of words can be highly context-dependent. Its variants like RoBERTa or ALBERT might offer improved performance or efficiency. **Seems BERT is a good start to try**\n",
    "\n",
    "DistilBERT: If computational resources or inference time are a concern, DistilBERT offers a good balance between performance and efficiency, as it is a distilled version of BERT that retains most of its capabilities. **Skip this for the time being, let us consider pure version of BERT first**\n",
    "\n",
    "GPT-3 or GPT-4: If your task can benefit from a large-scale pre-trained model and you have access to it, GPT-3 or GPT-4 can be fine-tuned for binary classification tasks. These models can be particularly powerful if your task involves creative language or requires a deep understanding of nuanced text. **Not very relevant to this task**\n",
    "\n",
    "Fine-tuning vs. Feature-based Approach: With models like BERT, you can either fine-tune the entire model on your task or use it as a feature extractor where only a simple classifier is trained on top of the BERT features. Fine-tuning generally offers better performance but requires more computational resources.\n",
    "\n",
    "Data Efficiency: If you have a small dataset, you might want to consider models that are known for being data-efficient. Techniques like few-shot learning with GPT-3 or GPT-4, or using a model pre-trained in a similar domain to your task, can be beneficial.\n",
    "\n",
    "Transfer Learning and Domain-Specific Models: If there's a pre-trained model that's been trained on a corpus relevant to your task (like legal documents, medical reports, etc.), using that model could lead to better performance as it's already familiar with the kind of language used in your domain.\n",
    "\n",
    "Here, we got relatively small size of training set, therefore BERT maybe the first one we want to try.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data clean for BERT model\n",
    "When fine-tuning a BERT model, it's generally a good idea to clean and preprocess your text data to ensure that it is in a format that is compatible with the model and conducive to effective learning. Here are some common preprocessing steps for training a BERT model.\n",
    "\n",
    "Text Cleaning: Depending on your dataset, you might want to remove unnecessary characters, such as HTML tags, special characters, or extra whitespace. You should also consider whether to convert the text to lowercase, as BERT models are available in both cased and uncased versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  Our Deeds are the Reason of this #earthquake M...   \n",
      "1             Forest fire near La Ronge Sask. Canada   \n",
      "2  All residents asked to 'shelter in place' are ...   \n",
      "3  13,000 people receive #wildfires evacuation or...   \n",
      "4  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  our deeds are the reason of this earthquake ma...  \n",
      "1              forest fire near la ronge sask canada  \n",
      "2  all residents asked to shelter in place are be...  \n",
      "3  people receive wildfires evacuation orders in ...  \n",
      "4  just got sent this photo from ruby alaska as s...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove usernames (mentions)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (just the symbol, not the text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Remove non-alphabetic characters and keep only the words\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Assuming the dataset is in a CSV file and the text is in a column named 'text'\n",
    "# Load your dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Clean the text column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Show the first few rows of the cleaned text\n",
    "print(df[['text', 'cleaned_text']].head())\n",
    "\n",
    "# Save the cleaned dataset to a new CSV file\n",
    "df.to_csv('cleaned_train.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding of BERT and its controlling parameters\n",
    "\n",
    "### BERT\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a model that uses the Transformer architecture. It was introduced by researchers at Google in 2018 and has since become one of the most popular and influential models in natural language processing (NLP).\n",
    "\n",
    "Here's a high-level overview of how the BERT model works:\n",
    "\n",
    "Input Representation: BERT takes as input a sequence of tokens (words or subwords) and converts them into vectors using an embedding layer. It also adds special tokens like [CLS] and [SEP] to the sequence and uses positional embeddings to capture the order of the tokens.\n",
    "\n",
    "Transformer Encoder: The core of the BERT model is a stack of Transformer encoder layers. Each layer consists of two main components: a multi-head self-attention mechanism and a feed-forward neural network. The self-attention mechanism allows the model to weigh the importance of different tokens in the sequence relative to each other, and the feed-forward network processes the output of the attention mechanism.\n",
    "\n",
    "Self-Attention: In the self-attention mechanism, each token in the input sequence is transformed into a query, key, and value vector. The model then computes attention scores by taking the dot product of the query vector of each token with the key vectors of all other tokens. These scores determine how much attention each token should pay to every other token in the sequence. The attention scores are then used to compute a weighted sum of the value vectors, which becomes the output of the attention mechanism.\n",
    "\n",
    "Feed-Forward Network: The output of the self-attention mechanism is then passed through a feed-forward neural network, which applies additional transformations to the data.\n",
    "\n",
    "Output: The output of the final Transformer encoder layer can be used for various NLP tasks. For example, in classification tasks, the output corresponding to the [CLS] token is often used as the representation of the entire sequence and passed through additional layers to produce the final class predictions.\n",
    "\n",
    "BERT's ability to capture the context of each token in a sequence bidirectionally (i.e., considering both the preceding and following tokens) is one of its key strengths. This allows it to understand the meaning of words in context more effectively than previous models that processed text in a unidirectional manner.\n",
    "\n",
    "### Training process\n",
    "Tokenization: Use the BERT tokenizer to tokenize your text. This involves splitting the text into words or subwords (tokens) and converting each token into its corresponding ID in the BERT vocabulary. The tokenizer will also add special tokens like [CLS] and [SEP] as needed.\n",
    "\n",
    "Padding and Truncation: Since BERT models require fixed-length input sequences, you'll need to pad shorter sequences and truncate longer ones to a specified maximum length. The BERT tokenizer can handle this for you.\n",
    "\n",
    "Attention Masks: Generate attention masks to tell the model which tokens are actual words and which are padding tokens. This is important for the model to correctly interpret the input sequences.\n",
    "\n",
    "Label Encoding: If you're working on a classification task, you'll need to encode your labels into a format that the model can understand (e.g., converting categorical labels into numerical IDs).\n",
    "\n",
    "### Controlling parameters (which used in the coding below)\n",
    "- add_special_tokens=True:\n",
    "\n",
    "This parameter tells the tokenizer to add special tokens to the beginning and end of each sequence. For BERT, these are typically [CLS] at the beginning and [SEP] at the end. The [CLS] token is used for classification tasks, and the [SEP] token is used to separate different sentences or segments within a single sequence.\n",
    "\n",
    "- return_attention_mask=True:\n",
    "\n",
    "This parameter tells the tokenizer to return the attention mask, which we discussed earlier. This is necessary for the BERT model to distinguish between real tokens and padding tokens.\n",
    "\n",
    "- pad_to_max_length=True:\n",
    "\n",
    "This parameter tells the tokenizer to pad all sequences to a specified maximum length (max_length). This is important because BERT models require all input sequences to be of the same length.\n",
    "\n",
    "- input_ids = inputs['input_ids']:\n",
    "\n",
    "input_ids are the tokenized representations of your input text. The BERT tokenizer converts each token (word or subword) in your text into a unique integer ID. These IDs are used by the BERT model to look up the corresponding embeddings (vector representations) of each token. The inputs['input_ids'] is a dictionary key that retrieves these tokenized input IDs from the tokenizer output.\n",
    "\n",
    "- attention_masks = inputs['attention_mask']:\n",
    "\n",
    "attention_masks are used to tell the model which tokens in the input are actual words and which ones are padding tokens. This is important because BERT models are trained on fixed-length sequences, and not all input sequences are the same length. Padding tokens (usually represented by the ID 0) are added to the end of shorter sequences to make them all the same length. The attention mask is a binary mask that indicates which tokens are padding (0) and which are real words (1).\n",
    "\n",
    "- output_attentions=False:\n",
    "\n",
    "This parameter is used when loading the BERT model. Setting it to False means that the model will not return the attention weights. The attention weights are used to understand how the model is focusing on different parts of the input sequence when making predictions. If you don't need this information, you can set this parameter to False to save memory and computation.\n",
    "\n",
    "- output_hidden_states=False:\n",
    "\n",
    "Similar to output_attentions, this parameter is used when loading the BERT model. Setting it to False means that the model will not return the hidden states of each layer. The hidden states can be used for more advanced analysis or for creating more complex models, but if you're just doing simple classification, you can set this to False to save resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_train.csv')\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sequences in the dataset\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    df['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=256,  # Choose a max_length that suits your dataset\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "# Get the input IDs and attention masks from the tokenizer output\n",
    "input_ids = inputs['input_ids']\n",
    "attention_masks = inputs['attention_mask']\n",
    "\n",
    "# Get the labels from the dataframe\n",
    "labels = torch.tensor(df['target'].values)\n",
    "\n",
    "# Load the BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,  # Number of output labels--2 for binary classification\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting Up Data Loaders\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Create a 90-10 train-validation split.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "AdamW is a popular optimizer used in training deep learning models, especially in the context of fine-tuning models like BERT. It is a variant of the Adam optimizer that incorporates weight decay, a technique used to regularize and prevent overfitting in the model.\n",
    "\n",
    "Here's why AdamW is commonly used for fine-tuning BERT:\n",
    "\n",
    "Adaptive Learning Rates: AdamW, like Adam, computes adaptive learning rates for each parameter. This means that it adjusts the learning rate based on the history of gradients for each parameter, which can lead to more effective and faster training compared to optimizers with a fixed learning rate.\n",
    "\n",
    "Weight Decay: AdamW incorporates weight decay in a way that is more compatible with adaptive learning rate algorithms. Weight decay is a form of regularization that helps prevent the weights from growing too large, which can reduce overfitting. In traditional optimizers like SGD with momentum, weight decay is applied directly to the weights, but in AdamW, it is decoupled from the gradient updates, which can lead to better performance in practice.\n",
    "\n",
    "Stability: AdamW includes a term called epsilon (eps) that helps improve numerical stability during optimization. This can be particularly important when working with deep neural networks, where small numerical errors can accumulate and lead to instability.\n",
    "\n",
    "While AdamW is a popular choice for fine-tuning BERT and other transformer-based models, there are other optimizers you could consider:\n",
    "\n",
    "SGD (Stochastic Gradient Descent): A simple and classic optimizer that can work well with a properly tuned learning rate and momentum. It may require more epochs to converge compared to AdamW.\n",
    "\n",
    "RMSprop: Similar to Adam in that it maintains a moving average of squared gradients to adapt the learning rate for each parameter, but does not have a bias correction term.\n",
    "\n",
    "Adam: The original version of AdamW, which also adapts learning rates based on the history of gradients but handles weight decay differently.\n",
    "\n",
    "Each optimizer has its strengths and weaknesses, and the best choice can depend on the specific task and dataset. However, AdamW is often recommended for fine-tuning BERT due to its balance of efficiency, stability, and effectiveness in handling sparse gradients and adaptive learning rates.\n",
    "\n",
    "\n",
    "### Discussion of optimizers\n",
    "\n",
    "Regarding whether AdamW is better than other optimizers, it's not always the case that AdamW is universally superior. Its effectiveness can depend on the specific task, model architecture, and dataset. However, AdamW is often preferred for training deep learning models like BERT and CNNs because it combines the benefits of adaptive learning rates (from Adam) with a more effective handling of weight decay. This can lead to better generalization and faster convergence in many cases.\n",
    "\n",
    "For CNNs, AdamW can also be a good choice, especially in scenarios where you're dealing with sparse gradients or require adaptive learning rates. However, traditional optimizers like SGD with momentum are also commonly used for training CNNs and can perform very well, particularly with a well-tuned learning rate schedule.\n",
    "\n",
    "In summary, while AdamW is a powerful optimizer that is well-suited for many deep learning tasks, the best optimizer for a given problem depends on the specific characteristics of the task and the model. It's often a good idea to experiment with different optimizers and learning rate schedules to find the best combination for your particular application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer & Learning Rate Scheduler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Set up the optimizer.\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5,    # This is the learning rate recommended by the BERT authors\n",
    "                  eps = 1e-8    # This is the default epsilon value in AdamW\n",
    "                )\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "epochs = 3\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Set up the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does 'Scheduler' do?\n",
    "The learning rate scheduler is used to adjust the learning rate during training, typically reducing it over time. This can help improve the performance of the model and ensure that it converges to a good solution. The scheduler you're using, get_linear_schedule_with_warmup, is a common choice for fine-tuning BERT and other transformer models. It starts with a \"warm-up\" phase, where the learning rate increases linearly from 0 to the initial learning rate (set when configuring the optimizer). After the warm-up phase, the learning rate decreases linearly from the initial learning rate to 0 over the remaining training steps. This approach helps stabilize the training process in the early stages and leads to better final performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions\n",
    "**flat_accuracy function**:\n",
    "The flat_accuracy function is used to calculate the accuracy of the model's predictions. Here's how it works:\n",
    "\n",
    "- preds is a 2D array where each row represents a prediction made by the model for a single input, and each column represents the probability of each class. The shape of preds is (number of examples, number of classes).\n",
    "\n",
    "- np.argmax(preds, axis=1) finds the index of the maximum value in each row, which corresponds to the predicted class. This converts the probabilities into class predictions.\n",
    "- pred_flat is a 1D array containing the predicted classes for all input examples.\n",
    "- labels is a 2D array containing the true labels, which are then flattened into a 1D array labels_flat.\n",
    "- The function then compares pred_flat and labels_flat element-wise to determine how many predictions match the true labels. The sum of matches is divided by the total number of examples to calculate the accuracy.\n",
    "\n",
    "**Explanation of model.cuda()**:\n",
    "\n",
    "model.cuda() is a method that moves the model's parameters and buffers to the GPU. This allows the model to take advantage of the parallel processing power of the GPU, which can significantly speed up training and inference.\n",
    "If you've only worked with CPU computation before, using .cuda() is a way to switch to GPU computation. Note that you'll need a CUDA-capable GPU and the appropriate CUDA software installed for this to work. If you don't have a GPU, you can remove this line, and the model will run on the CPU by default. **You will be introduced how to setup CUDA environment later**\n",
    "\n",
    "**model.train() and training loop**\n",
    "\n",
    "model.train() is a method that sets the model to training mode. This is important because some layers, like dropout and batch normalization, behave differently during training than during evaluation. By calling model.train(), you're telling the model that it should prepare for training.The for loop after model.train() iterates over the training data in batches. Here's what happens inside the loop:\n",
    "\n",
    "- Data Loading: Each batch of data is unpacked into input IDs (b_input_ids), attention masks (b_input_mask), and labels (b_labels). These are then moved to the GPU if available.\n",
    "- Zero Gradient: The gradients are zeroed out using model.zero_grad() to ensure that they don't accumulate across batches.\n",
    "- Forward Pass: The model performs a forward pass on the input data (b_input_ids and b_input_mask) and computes the loss based on the predictions and true labels (b_labels).\n",
    "- Backward Pass: The loss.backward() call computes the gradients of the loss with respect to the model parameters.\n",
    "- Gradient Clipping: torch.nn.utils.clip_grad_norm_() is used to prevent the gradients from becoming too large, which can cause numerical instability. The norm is a measure of the vector's magnitude, typically calculated using the L2 norm (Euclidean norm), which is the square root of the sum of the squares of all elements in the vector. If the norm of the gradient vector exceeds the specified threshold (in this case, 1), we scale down the entire vector so that its norm becomes equal to the threshold. This is done by dividing each element of the gradient vector by the ratio of the norm to the threshold. So, if the norm of the gradient vector is 1.9 and the threshold is 1, each element of the gradient vector is scaled down by a factor of 1/1.9. This ensures that the scaled gradient vector has a norm of 1 while preserving the direction of the original gradient vector. The elements of the gradient vector are scaled down proportionally, not just the highest value.\n",
    "- Parameter Update: The optimizer (optimizer.step()) updates the model parameters based on the computed gradients.\n",
    "- Learning Rate Update: The learning rate scheduler (scheduler.step()) adjusts the learning rate according to the schedule.\n",
    "\n",
    "This process is repeated for each batch in the dataset, and for each epoch (full pass over the dataset), allowing the model to learn from the data and adjust its parameters to minimize the loss.\n",
    "\n",
    "\n",
    "### Validation Phase \n",
    "- Add batch to GPU:\n",
    "\n",
    "The line batch = tuple(t.cuda() for t in batch) moves each tensor in the batch (input IDs, attention masks, and labels) to the GPU. This is done to accelerate computation since GPUs are much faster than CPUs for the matrix operations involved in deep learning.\n",
    "\n",
    "The batch is in tuple format because the data loader typically returns batches as tuples or lists of tensors. The tuple format is used here to ensure that the structure of the batch remains unchanged when moving it to the GPU.\n",
    "\n",
    "- Logits:\n",
    "\n",
    "In the context of neural networks, logits are the raw, unnormalized scores output by the last layer of the model before any activation function like softmax is applied. These scores are then typically passed through a softmax function to convert them into probabilities for each class.\n",
    "outputs[0] refers to the logits because, in the PyTorch implementation of BERT, the model returns a tuple where the first element (outputs[0]) is the logits for each input sequence. \n",
    "\n",
    "- Move logits and labels to CPU:\n",
    "\n",
    "Logits and labels are moved to the CPU (logits.detach().cpu().numpy() and b_labels.to('cpu').numpy()) for further processing (like calculating the F1 score) that doesn't require the computational power of the GPU. This is done because some operations, like converting tensors to NumPy arrays, can only be done on CPU tensors. Additionally, moving data off the GPU can help free up GPU memory for other computations.\n",
    "\n",
    "- Weighted average for F1 score:\n",
    "\n",
    "The average='weighted' parameter in the f1_score function specifies that the F1 scores for each class should be weighted by the number of true instances for each class. This means that the F1 score for each class is multiplied by the number of true instances in that class, and then the average is taken. This approach is useful in imbalanced datasets, where some classes might have significantly more instances than others. It ensures that the F1 score takes into account the class imbalance by giving more weight to the classes with more instances.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  215.    Elapsed: 0:00:33.\n",
      "  Batch 80  of  215.    Elapsed: 0:01:06.\n",
      "  Batch 120  of  215.    Elapsed: 0:01:40.\n",
      "  Batch 160  of  215.    Elapsed: 0:02:13.\n",
      "  Batch 200  of  215.    Elapsed: 0:02:46.\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:02:58\n",
      "\n",
      "Running Validation with F1 score...\n",
      "F1 Score: 0.83\n",
      "Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  215.    Elapsed: 0:00:33.\n",
      "  Batch 80  of  215.    Elapsed: 0:01:07.\n",
      "  Batch 120  of  215.    Elapsed: 0:01:40.\n",
      "  Batch 160  of  215.    Elapsed: 0:02:14.\n",
      "  Batch 200  of  215.    Elapsed: 0:02:47.\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "Running Validation with F1 score...\n",
      "F1 Score: 0.83\n",
      "Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "  Batch 40  of  215.    Elapsed: 0:00:33.\n",
      "  Batch 80  of  215.    Elapsed: 0:01:07.\n",
      "  Batch 120  of  215.    Elapsed: 0:01:40.\n",
      "  Batch 160  of  215.    Elapsed: 0:02:14.\n",
      "  Batch 200  of  215.    Elapsed: 0:02:47.\n",
      "  Average training loss: 0.06\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "Running Validation with F1 score...\n",
      "F1 Score: 0.83\n",
      "Validation took: 0:00:07\n",
      "\n",
      "Validation complete!\n"
     ]
    }
   ],
   "source": [
    "#Training loops\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function for formatting the elapsed time.\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "# Move the model to the GPU.\n",
    "model.cuda()\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "loss_values = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "    print('Training...')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'  Batch {step}  of  {len(train_dataloader)}.    Elapsed: {elapsed}.')\n",
    "        \n",
    "        # Unpack this training batch from our dataloader. \n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_input_mask = batch[1].cuda()\n",
    "        b_labels = batch[2].cuda()\n",
    "        \n",
    "        # Clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()        \n",
    "        \n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Accumulate the training loss over all of the batches \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {format_time(time.time() - t0)}\")    \n",
    "    print(\"\\nRunning Validation with F1 score...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Variables to gather full output\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.cuda() for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "        logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    # Flatten the predictions and true labels\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Convert logits to predicted class (0 or 1) using argmax\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1 = f1_score(flat_true_labels, flat_predictions, average='weighted') # 'weighted' accounts for label imbalance\n",
    "\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"Validation took: {format_time(time.time() - t0)}\\n\")\n",
    "\n",
    "print(\"Validation complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_bert_model3\\\\tokenizer_config.json',\n",
       " './my_bert_model3\\\\special_tokens_map.json',\n",
       " './my_bert_model3\\\\vocab.txt',\n",
       " './my_bert_model3\\\\added_tokens.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./my_bert_model3')\n",
    "tokenizer.save_pretrained('./my_bert_model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('my_bert_model3')\n",
    "model = BertForSequenceClassification.from_pretrained('my_bert_model3')\n",
    "\n",
    "input_text = df_test['text'].tolist()\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Convert the tensor to a NumPy array\n",
    "predictions_np = predictions.numpy()\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame\n",
    "predictions_df = pd.DataFrame(predictions_np, columns=['target'])\n",
    "\n",
    "df_output = pd.DataFrame()\n",
    "df_output['id'] = df_test['id']\n",
    "df_output['target'] = predictions_df['target']\n",
    "df_output.to_csv('submit3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu116\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal notesï¼š å…³äºBERTæ¨¡å‹å…¶ä»–ç±»å‹çš„fine-tune\n",
    "BERTæ¨¡å‹å¯ä»¥ç”¨äºå¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºäºŒåˆ†ç±»é—®é¢˜ã€‚å¯¹äºä¸åŒçš„ä»»åŠ¡ï¼Œæ¨¡å‹çš„å¾®è°ƒå’Œè¾“å‡ºå±‚å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼š\n",
    "\n",
    "å¤šåˆ†ç±»é—®é¢˜ï¼šå¦‚æœä»»åŠ¡æ˜¯å¤šåˆ†ç±»è€Œä¸æ˜¯äºŒåˆ†ç±»ï¼Œä½ å¯ä»¥ç®€å•åœ°ä¿®æ”¹æ¨¡å‹çš„è¾“å‡ºå±‚ï¼Œä½¿å…¶å…·æœ‰ä¸ç±»åˆ«æ•°é‡ç›¸å¯¹åº”çš„è¾“å‡ºç¥ç»å…ƒã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰10ä¸ªç±»åˆ«ï¼Œé‚£ä¹ˆè¾“å‡ºå±‚åº”è¯¥æœ‰10ä¸ªç¥ç»å…ƒï¼Œå¹¶ä¸”ä½¿ç”¨softmaxæ¿€æ´»å‡½æ•°æ¥å°†è¾“å‡ºè½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "ä¸Šä¸‹æ–‡é¢„æµ‹ï¼šå¦‚æœä»»åŠ¡æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡é¢„æµ‹å•è¯ï¼ˆä¾‹å¦‚ï¼Œå¡«ç©ºä»»åŠ¡ï¼‰ï¼Œä½ å¯ä»¥ä½¿ç”¨BERTçš„Masked Language Model (MLM) åŠŸèƒ½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ä¼šåœ¨è¾“å…¥æ–‡æœ¬ä¸­éšæœºé®ç›–ä¸€äº›å•è¯ï¼Œç„¶åè®­ç»ƒæ¨¡å‹æ¥é¢„æµ‹è¿™äº›é®ç›–çš„å•è¯ã€‚è¾“å‡ºå±‚éœ€è¦è°ƒæ•´ä¸ºè¯æ±‡è¡¨å¤§å°çš„è¾“å‡ºï¼Œä»¥é¢„æµ‹æ¯ä¸ªé®ç›–å•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "é—®ç­”ä»»åŠ¡ï¼šå¯¹äºé—®ç­”ä»»åŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨BERTæ¥åŒæ—¶é¢„æµ‹ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸä½ç½®ã€‚è¿™é€šå¸¸æ¶‰åŠåˆ°åœ¨BERTçš„åŸºç¡€ä¸Šæ·»åŠ ä¸¤ä¸ªè¾“å‡ºå±‚ï¼Œä¸€ä¸ªç”¨äºé¢„æµ‹ç­”æ¡ˆå¼€å§‹çš„ä½ç½®ï¼Œå¦ä¸€ä¸ªç”¨äºé¢„æµ‹ç­”æ¡ˆç»“æŸçš„ä½ç½®ã€‚æ¯ä¸ªè¾“å‡ºå±‚éƒ½ä¼šè¾“å‡ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œè¡¨ç¤ºç­”æ¡ˆå¼€å§‹æˆ–ç»“æŸçš„æ¯ä¸ªä½ç½®çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "å¯¹äºä»¥ä¸Šä»»ä½•ä»»åŠ¡ï¼Œæ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†ï¼ˆå³BERTçš„ç¼–ç å™¨å±‚ï¼‰é€šå¸¸ä¿æŒä¸å˜ï¼Œåªéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´è¾“å‡ºå±‚å’ŒæŸå¤±å‡½æ•°ã€‚æ­¤å¤–ï¼Œä½ å¯èƒ½è¿˜éœ€è¦æ ¹æ®ä»»åŠ¡çš„å…·ä½“éœ€æ±‚è°ƒæ•´è¾“å…¥æ•°æ®çš„æ ¼å¼å’Œé¢„å¤„ç†æ­¥éª¤ã€‚\n",
    "\n",
    "å¯¹äºé—®ç­”ï¼ˆQAï¼‰ä»»åŠ¡ï¼Œæ¨¡å‹é€šå¸¸è¢«è®­ç»ƒæ¥é¢„æµ‹ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸä½ç½®ã€‚è¿™é‡Œä»¥BERTä¸ºä¾‹æ¥è¯¦ç»†è¯´æ˜è®­ç»ƒè¿‡ç¨‹ï¼š\n",
    "\n",
    "æ•°æ®å‡†å¤‡ï¼š\n",
    "\n",
    "é¦–å…ˆï¼Œä½ éœ€è¦å‡†å¤‡ä¸€ä¸ªé—®ç­”æ•°æ®é›†ï¼Œå…¶ä¸­æ¯ä¸ªæ ·æœ¬åŒ…å«ä¸€ä¸ªé—®é¢˜ã€ä¸€ä¸ªåŒ…å«ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªæ®µè½ï¼‰ï¼Œä»¥åŠç­”æ¡ˆåœ¨ä¸Šä¸‹æ–‡ä¸­çš„å¼€å§‹å’Œç»“æŸä½ç½®ã€‚\n",
    "å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œä½ å°†é—®é¢˜å’Œä¸Šä¸‹æ–‡æ–‡æœ¬æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä¸­é—´ç”¨ä¸€ä¸ªç‰¹æ®Šåˆ†éš”ç¬¦ï¼ˆå¦‚[SEP]ï¼‰åˆ†éš”ï¼ŒåŒæ—¶åœ¨åºåˆ—çš„å¼€å§‹æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„åˆ†ç±»æ ‡è®°ï¼ˆå¦‚[CLS]ï¼‰ã€‚\n",
    "æ¨¡å‹æ¶æ„ï¼š\n",
    "\n",
    "ä½¿ç”¨BERTæ¨¡å‹å¯¹æ‹¼æ¥åçš„é—®é¢˜å’Œä¸Šä¸‹æ–‡æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°åºåˆ—çš„è¡¨ç¤ºã€‚\n",
    "åœ¨BERTçš„è¾“å‡ºå±‚ä¹‹ä¸Šï¼Œæ·»åŠ ä¸¤ä¸ªçº¿æ€§å±‚ï¼Œä¸€ä¸ªç”¨äºé¢„æµ‹ç­”æ¡ˆçš„å¼€å§‹ä½ç½®ï¼Œå¦ä¸€ä¸ªç”¨äºé¢„æµ‹ç­”æ¡ˆçš„ç»“æŸä½ç½®ã€‚è¿™ä¸¤ä¸ªçº¿æ€§å±‚çš„è¾“å‡ºç»´åº¦ç­‰äºåºåˆ—é•¿åº¦ï¼Œæ¯ä¸ªä½ç½®çš„è¾“å‡ºå€¼è¡¨ç¤ºè¯¥ä½ç½®æ˜¯ç­”æ¡ˆå¼€å§‹æˆ–ç»“æŸçš„æ¦‚ç‡ã€‚\n",
    "è®­ç»ƒè¿‡ç¨‹ï¼š\n",
    "\n",
    "åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æµ‹çš„å¼€å§‹å’Œç»“æŸä½ç½®ä¸çœŸå®å¼€å§‹å’Œç»“æŸä½ç½®ä¹‹é—´çš„å·®è·ã€‚è¿™é€šå¸¸é€šè¿‡äº¤å‰ç†µæŸå¤±å‡½æ•°æ¥å®ç°ã€‚\n",
    "å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„æŸå¤±ï¼Œç„¶åå°†è¿™ä¸¤ä¸ªæŸå¤±ç›¸åŠ å¾—åˆ°æ€»æŸå¤±ã€‚ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼ˆå¦‚Adamï¼‰æ¥æ›´æ–°æ¨¡å‹å‚æ•°ï¼Œä»¥æœ€å°åŒ–æ€»æŸå¤±ã€‚\n",
    "é¢„æµ‹å’Œè¯„ä¼°ï¼š\n",
    "\n",
    "åœ¨é¢„æµ‹é˜¶æ®µï¼Œæ¨¡å‹æ¥æ”¶ä¸€ä¸ªé—®é¢˜å’Œä¸€ä¸ªä¸Šä¸‹æ–‡æ–‡æœ¬ï¼Œè¾“å‡ºç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸä½ç½®çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ä½ç½®ä½œä¸ºç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸä½ç½®ã€‚ç„¶åä»ä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­æå–å‡ºè¿™ä¸ªèŒƒå›´å†…çš„æ–‡æœ¬ä½œä¸ºé¢„æµ‹ç­”æ¡ˆã€‚\n",
    "æ¨¡å‹çš„æ€§èƒ½é€šå¸¸é€šè¿‡è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰ã€å¬å›ç‡ï¼ˆRecallï¼‰å’ŒF1åˆ†æ•°æ¥è¡¡é‡ï¼Œè¿™äº›æŒ‡æ ‡è€ƒè™‘äº†é¢„æµ‹ç­”æ¡ˆä¸çœŸå®ç­”æ¡ˆçš„é‡å ç¨‹åº¦ã€‚\n",
    "è¿™å°±æ˜¯ä½¿ç”¨BERTè¿›è¡Œé—®ç­”ä»»åŠ¡çš„åŸºæœ¬è®­ç»ƒè¿‡ç¨‹ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦å¯¹è¿™ä¸ªè¿‡ç¨‹è¿›è¡Œè°ƒæ•´ï¼Œä»¥é€‚åº”ç‰¹å®šçš„æ•°æ®é›†å’Œä»»åŠ¡è¦æ±‚ã€‚\n",
    "\n",
    "\n",
    "åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç¡®å®å­˜åœ¨è¿™æ ·çš„é—®ç­”ä»»åŠ¡ï¼Œå…¶ä¸­åªæä¾›é—®é¢˜ï¼Œè€Œæ²¡æœ‰ç»™å‡ºåŒ…å«ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡ã€‚è¿™ç§ä»»åŠ¡é€šå¸¸è¢«ç§°ä¸ºå¼€æ”¾åŸŸé—®ç­”ï¼ˆOpen-domain Question Answeringï¼‰ï¼Œå®ƒæ¯”ä¼ ç»Ÿçš„åŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”ï¼ˆContext-based Question Answeringï¼‰æ›´å…·æŒ‘æˆ˜æ€§ã€‚å¼€æ”¾åŸŸé—®ç­”é€šå¸¸æ¶‰åŠä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "æ–‡æ¡£æ£€ç´¢ï¼ˆDocument Retrievalï¼‰ï¼šç»™å®šä¸€ä¸ªé—®é¢˜ï¼Œç³»ç»Ÿé¦–å…ˆéœ€è¦ä»ä¸€ä¸ªå¤§å‹çš„æ–‡æ¡£é›†åˆï¼ˆå¦‚ç»´åŸºç™¾ç§‘ï¼‰ä¸­æ£€ç´¢å‡ºç›¸å…³çš„æ–‡æ¡£æˆ–æ®µè½ã€‚è¿™ä¸ªæ­¥éª¤é€šå¸¸ä½¿ç”¨ä¿¡æ¯æ£€ç´¢ï¼ˆInformation Retrievalï¼‰æŠ€æœ¯æ¥å®ç°ï¼Œå¦‚å€’æ’ç´¢å¼•ï¼ˆInverted Indexï¼‰ã€TF-IDFã€BM25ç­‰ã€‚\n",
    "\n",
    "å€™é€‰ç”Ÿæˆï¼ˆCandidate Generationï¼‰ï¼šä»æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸­æå–å‡ºå¯èƒ½åŒ…å«ç­”æ¡ˆçš„æ–‡æœ¬ç‰‡æ®µä½œä¸ºç­”æ¡ˆå€™é€‰ã€‚è¿™å¯ä»¥é€šè¿‡ç®€å•çš„å¯å‘å¼æ–¹æ³•æ¥å®ç°ï¼Œä¾‹å¦‚é€‰æ‹©åŒ…å«é—®é¢˜ä¸­å…³é”®è¯çš„å¥å­æˆ–æ®µè½ã€‚\n",
    "\n",
    "ç­”æ¡ˆæŠ½å–ï¼ˆAnswer Extractionï¼‰ï¼šå¯¹äºæ¯ä¸ªå€™é€‰æ–‡æœ¬ï¼Œä½¿ç”¨ç±»ä¼¼äºä¸Šè¿°åŸºäºä¸Šä¸‹æ–‡çš„é—®ç­”æ¨¡å‹ï¼ˆå¦‚åŸºäºBERTçš„æ¨¡å‹ï¼‰æ¥é¢„æµ‹ç­”æ¡ˆçš„å¼€å§‹å’Œç»“æŸä½ç½®ã€‚è¿™ä¸ªæ­¥éª¤é€šå¸¸éœ€è¦å°†é—®é¢˜å’Œå€™é€‰æ–‡æœ¬æ‹¼æ¥èµ·æ¥ï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚\n",
    "\n",
    "ç­”æ¡ˆæ’åºï¼ˆAnswer Rankingï¼‰ï¼šå¯¹æ‰€æœ‰å€™é€‰ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œé€‰æ‹©æœ€å¯èƒ½çš„ç­”æ¡ˆä½œä¸ºæœ€ç»ˆè¾“å‡ºã€‚æ’åºå¯ä»¥åŸºäºæ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ï¼Œä¹Ÿå¯ä»¥ç»“åˆå…¶ä»–ç‰¹å¾ï¼Œå¦‚å€™é€‰æ–‡æœ¬çš„ç›¸å…³æ€§å¾—åˆ†ã€ç­”æ¡ˆçš„ç½®ä¿¡åº¦ç­‰ã€‚\n",
    "\n",
    "å¼€æ”¾åŸŸé—®ç­”éœ€è¦ç»¼åˆè¿ç”¨ä¿¡æ¯æ£€ç´¢ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œæœºå™¨å­¦ä¹ çš„æŠ€æœ¯ã€‚éšç€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œæ£€ç´¢æŠ€æœ¯çš„å‘å±•ï¼Œå¼€æ”¾åŸŸé—®ç­”ç³»ç»Ÿçš„æ€§èƒ½æ­£åœ¨é€æ¸æé«˜ï¼Œä½†ä»ç„¶æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€ä¸ªæ´»è·ƒç ”ç©¶æ–¹å‘ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
